{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacd966e",
   "metadata": {},
   "source": [
    "### Data Cleansing Overview\n",
    "**Data Cleaning** refers to identifying and correcting errors in the dataset that may negatively impact a predictive model\n",
    "* In structred data you can use lots of statistical analysis and data visualization techniques that we can use to explore our data in order to define the data cleansing operation that we want to perform.\n",
    "* There are some very simple yet crucial steps before more sophisticated techniques that may be overlooked and if they being skipped our model may break or perform overly optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1742b75",
   "metadata": {},
   "source": [
    "#### Step1: identify and delete Columns that contain a Single Value\n",
    "Example:\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "\n",
    "data = loadtxt('oil-spill.csv', delimiter=',')\n",
    "* summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tprint(i, len(unique(data[:, i])))\n",
    "**OR**\n",
    "from pandas import read_csv\n",
    "df = read_csv('oil-spill.csv', header=None)\n",
    "print(df.nunique())\n",
    "**Then**\n",
    "from pandas import read_csv\n",
    "* load the dataset\n",
    "df = read_csv('oil-spill.csv', header=None)\n",
    "print(df.shape)\n",
    "* get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "* record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "* drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033dca70",
   "metadata": {},
   "source": [
    "#### Step2: identify Columns with Few Values\n",
    "* Example\n",
    "* from numpy import loadtxt\n",
    "* from numpy import unique\n",
    "* data = loadtxt('oil-spill.csv', delimiter=',') \n",
    "* ''summarize the number of unique values in each column'' \n",
    "* for i in range(data.shape[1]):\n",
    "\tnum = len(unique(data[:, i]))\n",
    "\tpercentage = float(num) / data.shape[0] * 100\n",
    "\tif percentage < 1:\n",
    "\t\tprint('%d, %d, %.1f%%' % (i, num, percentage))\n",
    "*  ''delete columns where number of unique values is less than 1% of the rows'' \n",
    "* from pandas import read_csv \n",
    "*  ''load the dataset'' \n",
    "* df = read_csv('oil-spill.csv', header=None)\n",
    "    print(df.shape) \n",
    "* ''get number of unique values for each column'' \n",
    "* counts = df.nunique()\n",
    "* ''record columns to delete''\n",
    "* to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
    "* print(to_del)\n",
    "* ''drop useless columns''\n",
    "* df.drop(to_del, axis=1, inplace=True)\n",
    "* print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75ae58",
   "metadata": {},
   "source": [
    "#### Step3: Remove Columns with Low Variance\n",
    "* Example of applying the variance threshold for feature selection\n",
    "* from pandas import read_csv\n",
    "* from sklearn.feature_selection import VarianceThreshold\n",
    "*'' load the dataset''\n",
    "* df = read_csv('oil-spill.csv', header=None)\n",
    "* ''split data into inputs and outputs''\n",
    "* data = df.values\n",
    "* X = data[:, :-1]\n",
    "* y = data[:, -1]\n",
    "* print(X.shape, y.shape)\n",
    "* '' define the transform ''\n",
    "* transform = VarianceThreshold()\n",
    "* ''transform the input data''\n",
    "* X_sel = transform.fit_transform(X)\n",
    "* print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ac23a",
   "metadata": {},
   "source": [
    "### Step4: Identify and Remove Rows that contain duplicate data\n",
    "*  ''locate rows of duplicate data''\n",
    "* from pandas import read_csv\n",
    "* ''load the dataset''\n",
    "* df = read_csv('iris.csv', header=None)\n",
    "*  ''calculate duplicates''\n",
    "* dups = df.duplicated()\n",
    "* ''report if there are any duplicates''\n",
    "* print(dups.any())\n",
    "* ''list all duplicate rows''\n",
    "* print(df[dups])\n",
    "#### **OR even easier version**\n",
    "* ''delete rows of duplicate data from the dataset''\n",
    "* from pandas import read_csv\n",
    "* '' load the dataset''\n",
    "* df = read_csv('iris.csv', header=None)\n",
    "* print(df.shape)\n",
    "* ''delete duplicate rows''\n",
    "* df.drop_duplicates(inplace=True)\n",
    "* print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce1f75",
   "metadata": {},
   "source": [
    "### Step 5: Identifying and Removing Outliers\n",
    "* It is important to know not all outliers should be removed and it's kind of personal judgement for each dataset\n",
    "* for the quicl check a scatterplot would help a lot\n",
    "#### First Approach : IQR\n",
    "* ''identify outliers with interquartile range''\n",
    "* from numpy.random import seed\n",
    "* from numpy.random import randn\n",
    "* from numpy import percentile\n",
    "* ''seed the random number generator''\n",
    "* seed(1)\n",
    "* ''generate univariate observations''\n",
    "* data = 5 * randn(10000) + 50\n",
    "* ''calculate interquartile range''\n",
    "* q25, q75 = percentile(data, 25), percentile(data, 75)\n",
    "* iqr = q75 - q25\n",
    "* print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q25, q75, iqr))\n",
    "* ''calculate the outlier cutoff**\n",
    "* cut_off = iqr * 1.5\n",
    "* lower, upper = q25 - cut_off, q75 + cut_off\n",
    "* ''identify outliers''\n",
    "* outliers = [x for x in data if x < lower or x > upper]\n",
    "* print('Identified outliers: %d' % len(outliers))\n",
    "* ''remove outliers''\n",
    "* outliers_removed = [x for x in data if x >= lower and x <= upper]\n",
    "* print('Non-outlier observations: %d' % len(outliers_removed))\n",
    "#### Second Approach: Automatic Outlier Detection\n",
    "* look at related notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190dacb",
   "metadata": {},
   "source": [
    "### Step 6: Mark Missing Values\n",
    "* some columns may have 0 instaed of missing values first we should make sure whether 0 could be a true observation or not by using.describe() function and looking at the min and max. after we made sure that 0 are missing value then we can replace them ba nan using .replace(0, nan). \n",
    "#### First Approach: Removing missing values (not recommended especially with large number of nan or small datasets\n",
    "* we can use df.dropna(inplace=True).\n",
    "#### Second Approach: Statistical Imputation (mean: most common, mode, median)\n",
    "* from sklearn.impute import SimpleImputer\n",
    "* imputer = SimpleImputer(strategy='mean') *it can be changed to mode and median as well\n",
    "* imputer.fit(X)\n",
    "* Xtrain = imputer.transform(X)\n",
    "#### Third Approach : Simple Imputer with Model Evaluation\n",
    "* here we want to deal with missing value in train and test data seprately in order to prevent the data leakage for doing so we use *pipeline\n",
    "* model = could be any model\n",
    "* imputer = SimpleImputer(strategy='mean')\n",
    "* pipeline = Pipeline(steps=[('i', imputer), ('m', model)])\n",
    "#### Fourth Approach: Compare Different Statistical Imputation Strategies\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "* '' load dataset''\n",
    "* url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/horse-colic.csv'\n",
    "* dataframe = read_csv(url, header=None, na_values='?')\n",
    "* ''split into input and output elements''\n",
    "* data = dataframe.values\n",
    "* ix = [i for i in range(data.shape[1]) if i != 23]\n",
    "* X, y = data[:, ix], data[:, 23]\n",
    "* ''evaluate each strategy on the dataset''\n",
    "* results = list()\n",
    "* strategies = ['mean', 'median', 'most_frequent', 'constant']\n",
    "* for s in strategies:\n",
    "\t* ''create the modeling pipeline''\n",
    "\t* pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n",
    "\t* ''evaluate the model''\n",
    "\t* cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t* scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\t* ''store results''\n",
    "\t* results.append(scores)\n",
    "\t* print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "\t* ''create the modeling pipeline''\n",
    "\t* pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestClassifier())])\n",
    "\t* ''evaluate the model''\n",
    "\t* cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t* scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\t* ''store results''\n",
    "\t* results.append(scores)\n",
    "\t* print('>%s %.3f (%.3f)' % (s, mean(scores), std(scores)))\n",
    "* **After finding out which strategy works better then we can choose that startegy or our model**\n",
    "#### Fifth Approach: K-Nearest Neighbors Imputation (model that can predict missing values)\n",
    "* from sklearn.impute import KNNImputer\n",
    "* load dataset and divide it to X and Y\n",
    "* summarize total missing by *sum(isnan(X)flatten())\n",
    "* define imputer *imputer=KNNImputer()\n",
    "* fit on the dataset *imputer.fit(X)\n",
    "* transform dataset *Xtrans= imputer.transform(X)\n",
    "* summarite total missing *sum(isnan(Xtrans).flatten())\n",
    "* using pipeline same with previous approach just changing the imputer to **KNNImputer**\n",
    "    *model=RandomForestClassifier()\n",
    "    *imputer=KNNImputer\n",
    "    *pipeline=Pipeline(steps=[('i', imputer), ('m', model)]\n",
    "    *cv = REpeatedtratifedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    *scores= cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    *print('Mean Accuracy: %.3f (%.3f)'(mean(scores), std(scores)))\n",
    "* look at the related notebook for analyzing which Kfold works better\n",
    "#### Sixth Approach: Iterative Imputation \n",
    "* in this approach the machine try to fill missing values by looking at each feature as a function of another feature and its iterative because it keeps repeat this process until all missing values being filled\n",
    "* the most common model for doing that is linear regression as if we are trying to predict the missing value by using other columns(features)\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impue import IterativeImputer\n",
    "* defining imputer *imputer=IterativeImputer()\n",
    "* fit and then transform the date set \n",
    "    *imputer.fit(x)\n",
    "    *Xtrans=imputer.transform(X)\n",
    "* for implementing the IterativeImputer and Model Evaluation look at related Notebook\n",
    "* by default the Iterate Imputer repeat the number of iteration 10 times. It is possible the large number of iteration may begin to bias or skewed the estimate hence few iteration may be preffered to check which number of iteartion works better looks at the related notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11484c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
