{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a876dc20",
   "metadata": {},
   "source": [
    "**It is desired to reduce the number of features to reduce the computation cost of modeling and in many cases to improve the performance of model. There are lots of statistical models that can help us to find out wich features have strong relation to our target. Yet choosing these methods is highly dependent on datatype of both input and output variables.**\n",
    "* Future Selection Goals\n",
    "    + Two Core Types -> Supervised (Target variable) and Unsupervised (No Target Variable)\n",
    "    + Filter Based -> uses statistical measures to score the correlation and dependence between input variables that can be filtered to choose the most relevant feature\n",
    "    + Carefully Chosen -> the feature selection method most be carefully chosen based on the input and output data type  \n",
    "* Supervised Feature Selection Methods: Intrinsic, Filter, Wrapper (all belongs to Dimensionality Reduction Techniques)\n",
    "    + Intrinsic: Auto Feature Selection (algorithems during training-> Lasso and Decision Trees)\n",
    "    + Filter: Relationship to target (Use statistical techniques to find realtionship between feature and target)\n",
    "    + Wrapper: According to Model (search the subset of features that performs according to predictive model/computationaly expensive)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbddc45",
   "metadata": {},
   "source": [
    "#### Statistics for Feature Selection\n",
    "* Common Data Types\n",
    "    + Output variable: Numerical -> Regression / Categorical -> Classification \n",
    "    + Imput Variables(Features): for chart of method that can be used look at FS.png    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f1b6d1",
   "metadata": {},
   "source": [
    "### Categorical Feature Selection\n",
    "* for a dataset with only categorical data we use *OrdinalEncoder* for input variables (works for more than one variable) and *LabelEncoder* for output variable (single variable) by importing them from *sklearn.preprocessing*\n",
    "* There are two methods for feature selection for categorical variables: \n",
    "    + Chi-Squared: larger the Chi-squared better feature and vice versa. However there is no defined threshold for magnitude of Chi-squared for feature selection so the selction of feature is our own judgment/or by evaluating of model with different subset  \n",
    "    + Mutual Information: typical used in Decission Trees/Random Forest model\n",
    "* for looking at the deatil of procedure of each method looks at the 'Categorical Feature Selection' notebook\n",
    "* the best approach is to evaluate the model (Logestic Regression is the best model for evaluation the eature selection technique for categorical data) with each technique to see which one works better. For doing so you can automate the model evaluation process. For detail of how we can automate the process look at the 'Categorical Feature Selection' notebook\n",
    "* It is always to better to check the cross-validation instead or in addition to simple model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6991e2",
   "metadata": {},
   "source": [
    "### Feature Selection with ANOVA on Numerical Input with Categorical Output\n",
    "* The most common technique for Feature Selection for numerical inputs are:\n",
    "    + **ANOVA F-test**: ANOVA stands for **analysis of variance** and is parametric statistical hypothesis test for determining whether the means from two or more samples of data come from same distribution or not. An F-satatistic or **F-test**, is a class of statistical test that calculates the ratio between **variances values**, such as the variance rfom two different samples or the explained and unexplained variance by a statistical test, like ANOVA. ANOVA is used when variable is categorical and other is numerical. Similar to Chi-Squared the higher the F-Test higher relation between feature and target.  \n",
    "    + **Mutual Information**: from the field of infrmation theory is the application of information gain (typically used in construction of Decision Trees) to feature selection. Mutual Information is calculated between two variables and measures the reduction in uncertainty one variable given a known value of the other variable. Mutual Information is a straight forward, we consider the distribution of two discrete categorical or ordinal variable. such as categorical input and categorical output. Therefore it could be also adapted to use between numerical input and categorical output.    \n",
    "* the best approach is to evaluate the model (Logestic Regression is the best model for evaluation the eature selection technique for categorical data) with each technique to see which one works better. For doing so you can automate the model evaluation process.\n",
    "* for looking at the deatil of procedure of each method looks at the 'Choosing Numerical Input Features' notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c67e8",
   "metadata": {},
   "source": [
    "### Tuning Number of Selected Features\n",
    "* The best pratice to find which features should be used we can use the **GridSearchCV** function. For doing so we should combine the **RepeatedSartifieldKFold** for cross-validation then creating a pipeline by passing our Feature Selection Method together with the model and then building a dictionary for grid to pass all number of possible features through a for loop and then use a grid search to find the accuracy of each one of them.\n",
    "    + Step1: *cv= RepeatedSartifieldKFold(n_splits=10, n_repeats=3, random_state=1)*\n",
    "    + Step2: *model=LogisticRegression(solver='liblinear')*\n",
    "    + Step3: *fs=SelectKBest=(score_func=f_classif)\n",
    "    + Step4: *pipeline=Pipeline(steps=[('anova', fs), ('lr', model)])\n",
    "    + Step5: *grid=dict() -> gird['anova_k'] = [i+1 for i in range (X.shape[1])]\n",
    "    + Step6: *search=GridSearchCV(pipeline, grid, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "* However the best practice for tuning the number of features, is to additionaly looking at the relationship between each selected number of features with model accuracy by drawing the box-whisker plot and then decide the number of selected features.\n",
    "* for looking at the deatil of procedure of each method looks at the 'Choosing Numerical Input Features' notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d1229",
   "metadata": {},
   "source": [
    "### Select Features for Numerical Output\n",
    "* For selecting the number of features between numerical input and output variable(s) especially in case of linear regression. Because the strength of relation between input and output can be caluclated by correlation statistics or Mutual Information. \n",
    "    + **Correlation (Pearson):** we are interested to positive numbers which are close to 1. Hence we can convert the linear correlation to only with positive values statistics via importing *f_regression* from *sklearn.feature_selection*. However when we calculate the correlation with this method to final score is not a number between 0 and 1. Yet it is always a positive number. The higher the better.\n",
    "    + **Mutual Information**: when we have numerical output we should import *mutal_info_regression* from *sklearn.feature_selection*. same with correlation higher the score better the feature.\n",
    "* same with previous techniques we should test each method with model accuracy.\n",
    "* for looking at the deatil of procedure of each method looks at the 'Select Features for Numerical Output' notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dd8cf7",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination (RFE): \n",
    "*  RFE is a feature selection algorithm. RFE is a **Wrapper** style feature selection model. This means that different machine learning model was given and used as a core method wrapped by RFE and used to help select features. Now this is in contrast with basic feature selection techniques that score each feature and then select the beast feature with the largest or smallest score. Technically RFE is a wrapper style feature selection model that also uses further feature selection internally. RFE works by selecting f subset of features by starting all the futures in training dataset and then successfully removing features until the desired number of features remains. This is achieved by feeding the machine learning algorithm used by the core model and ranking features by their importance and discarding the least important features and then refeeding that model. This process then repeated until the specified numbers of features remains. Features are scored either using provided machine learning models i.e some algorithm such as Decission Trees or by using statistical methods. There are two configuration options for using RFE.       \n",
    "    + **Choice in the Number of Features**:\n",
    "    + **Choice of the Algorithm used to Select Features**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
